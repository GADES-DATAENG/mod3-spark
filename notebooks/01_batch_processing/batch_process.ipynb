{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4657f9d1-c83b-47c8-a455-36361c60b5c0",
   "metadata": {},
   "source": [
    "# Introduction to PySpark Basics\n",
    "\n",
    "## About This Notebook\n",
    "\n",
    "This notebook was designed to provide a hands-on introduction to **PySpark**, the Python API for **Apache Spark**, a powerful distributed data processing framework. The primary goal is to demonstrate how PySpark works, including key concepts like lazy transformations, parallel processing, and SparkSQL for querying data.\n",
    "\n",
    "---\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. **Spark Basics**: How to create a Spark session and load data into Spark DataFrames.\n",
    "2. **Lazy Transformations**: Understand how Spark optimizes execution by delaying computations until an action is triggered.\n",
    "3. **Actions vs. Transformations**: Learn the difference between these two core operations.\n",
    "4. **Parallelism in Spark**: See how Spark distributes data and processing across multiple nodes for better performance.\n",
    "5. **SparkSQL**: Use SQL queries to analyze data in Spark DataFrames and leverage the full power of distributed processing.\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster Setup\n",
    "\n",
    "For demonstration purposes, this notebook connects to a local Spark cluster running in cluster mode. Spark is configured to utilize multiple worker nodes to showcase distributed data processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690474a6-ab5f-40f5-b58a-80351f8f05d6",
   "metadata": {},
   "source": [
    "---\n",
    "### **Start by creating a PySpark application that will be used accross the entire Notebook**\n",
    "After running this command, the Spark Application will be up and running and you can check it by going to the [http://localhost:4040](http://localhost:4040) page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6343c28-de3f-48d6-862e-ccab37456717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/11 20:04:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with more detailed configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkTest\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"29417\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dfbe2-aa70-4056-b23f-e68fa9b78e7a",
   "metadata": {},
   "source": [
    "---\n",
    "### **Now let's load the dataset files into a PySpark Dataframe and check its schema as well as the first 10 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28920d2f-e422-49ee-9758-905b3b82c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp_ntz (nullable = true)\n",
      " |-- on_scene_datetime: timestamp_ntz (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:18:06|2023-01-01 00:19:24|2023-01-01 00:19:38|2023-01-01 00:48:07|          48|          68|      0.94|     1709|              25.95|  0.0|0.78|      2.3|                2.75|        0.0|5.22|     27.83|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:48:42|2023-01-01 00:56:20|2023-01-01 00:58:39|2023-01-01 01:33:08|         246|         163|      2.78|     2069|              60.14|  0.0| 1.8|     5.34|                2.75|        0.0| 0.0|     50.15|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:15:35|2023-01-01 00:20:14|2023-01-01 00:20:27|2023-01-01 00:37:54|           9|         129|      8.81|     1047|              24.37|  0.0|0.73|     2.16|                 0.0|        0.0| 0.0|     20.22|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:35:24|2023-01-01 00:39:30|2023-01-01 00:41:05|2023-01-01 00:48:16|         129|         129|      0.67|      431|               13.8|  0.0|0.41|     1.22|                 0.0|        0.0| 0.0|       7.9|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:43:15|2023-01-01 00:51:10|2023-01-01 00:52:47|2023-01-01 01:04:51|         129|          92|      4.38|      724|              20.49|  0.0|0.61|     1.82|                 0.0|        0.0| 0.0|     16.48|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B03406|                null|2023-01-01 00:21:34|               null|2023-01-01 00:29:05|2023-01-01 00:49:54|         130|          38|     4.921|     1249|              18.29|  0.0|0.43|     1.27|                 0.0|        0.0| 0.0|     16.81|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B03406|                null|2023-01-01 00:47:17|               null|2023-01-01 00:55:29|2023-01-01 01:16:07|          38|          10|     5.517|     1238|              25.76|  0.0|0.77|     2.29|                 0.0|        0.0| 0.0|     23.65|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:06:54|2023-01-01 00:08:59|2023-01-01 00:10:29|2023-01-01 00:18:22|          90|         231|      1.89|      473|              14.51|  0.0|0.44|     1.29|                2.75|        0.0| 0.0|     13.73|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:15:22|2023-01-01 00:21:39|2023-01-01 00:22:10|2023-01-01 00:33:14|         125|         246|      2.65|      664|               13.0|  0.0|0.39|     1.15|                2.75|        0.0| 0.0|     13.61|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:26:02|2023-01-01 00:39:09|2023-01-01 00:39:09|2023-01-01 01:03:50|          68|         231|      3.26|     1481|              30.38|  0.0|0.91|      2.7|                2.75|        0.0| 0.0|     28.25|                  N|                N|                  |               N|             Y|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Read the dataset into a PySpark Dataframe\n",
    "# As the file path is *parquet, PySpark will read all the files with the .parquet extension\n",
    "df = spark.read.parquet('file:///opt/spark/data/*.parquet')\n",
    "\n",
    "# Print the dataframe schema inferred from the .parquet files\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first 10 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e79c56-77bb-452e-b25d-07d22c341f87",
   "metadata": {},
   "source": [
    "---\n",
    "### **Transformations and Actions**\n",
    "In Spark we have these two fundamental types of operations:\n",
    "1. **Transformations**: Operations that run on RDDs, Dataframes or Datasets that produce a new distributed dataset from the existing one. They are lazy, which means that product an execution plan and does not actually execute the operation. The transformation (or transformations) applied on a Dataframe are just executed once an action is performed on the Dataframe. Examples are `map`, `filter`, `groupBy`, `join`, `repartition`\n",
    "2. **Actions**: Operations that trigger the execution of transformations and produce a value as return and not a new dataset or write the result to the storage. Actions are the operations that trigger data computation. Examples are `collect`, `count`, `show`, `first`, `write`\n",
    "\n",
    "In the next block, you will see how transformations and actions interact between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ce58d8-866c-4ff6-89fb-f94029d7dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a transformation example. It will select just 2 columns from the dataframe (pickup_location and trip_duration)\n",
    "# It will also filter the dataset by the rows where trip_duration > 30\n",
    "# After executing this command, no computation will be triggered and you can verify this in the Spark Application UI\n",
    "# You will not see any job beeing triggered in the Spark Application UI after running this command\n",
    "\n",
    "transformed_df = df.select(\"PULocationID\", \"trip_time\").filter(df.trip_time > 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261862b1-5353-4063-a03d-676be14dede9",
   "metadata": {},
   "source": [
    "Only after running an action on top of the `transformed_df` the transformation will be applied on the data. Just run the next command and you will see the computation being applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ab75ca-f784-4cea-8348-405bfb82d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|PULocationID|trip_time|\n",
      "+------------+---------+\n",
      "|          48|     1709|\n",
      "|         246|     2069|\n",
      "|           9|     1047|\n",
      "|         129|      431|\n",
      "|         129|      724|\n",
      "|         130|     1249|\n",
      "|          38|     1238|\n",
      "|          90|      473|\n",
      "|         125|      664|\n",
      "|          68|     1481|\n",
      "|          79|     2078|\n",
      "|         143|     2336|\n",
      "|          49|      771|\n",
      "|         181|      391|\n",
      "|          25|     2263|\n",
      "|         216|     1818|\n",
      "|         223|      580|\n",
      "|           7|      798|\n",
      "|         223|     1131|\n",
      "|          79|     1792|\n",
      "+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c97cf7-d642-4810-8c0e-d4613f523f82",
   "metadata": {},
   "source": [
    "---\n",
    "### **Aggregations**\n",
    "The next command will do some aggregation on top of the Dataframe:\n",
    "1. **Group**: Aggregate the data on top of `pickup_location`\n",
    "2. **Average**: Calculate the average on top of `trip_duration` and give an alias of `avg_duration`\n",
    "3. **Order**: The last transformation is ordering the resultset by `avg_duration` descending (`ascending=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d390a0f9-6f99-4e31-bcd5-0f21058fd3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|PULocationID| avg_trip_duration|\n",
      "+------------+------------------+\n",
      "|         132| 2353.454890823891|\n",
      "|         138|1747.5942813485483|\n",
      "|         199| 1612.388888888889|\n",
      "|         202|1460.9190927555856|\n",
      "|          46|1324.7144465290808|\n",
      "|           2|1321.4761904761904|\n",
      "|         261|1314.8798892350294|\n",
      "|         194|1310.3412698412699|\n",
      "|          88|1308.5389453422265|\n",
      "|         100|1304.4686264182608|\n",
      "|         230| 1278.785676513117|\n",
      "|          12|1271.1152312599681|\n",
      "|          87|         1265.6182|\n",
      "|         195|1260.3514171617805|\n",
      "|         117|1239.9431008248578|\n",
      "|         186|1237.4374664978286|\n",
      "|          27|     1232.54296875|\n",
      "|         161|1221.2275508871828|\n",
      "|         163|1218.4810828420916|\n",
      "|          48|1210.5463232368127|\n",
      "|          68|1207.6508681022256|\n",
      "|         162|1202.4361121152654|\n",
      "|         154| 1194.007332722273|\n",
      "|         140| 1188.507840248026|\n",
      "|         209|1183.5469328755985|\n",
      "|          33| 1183.322699088146|\n",
      "|         201|1179.5724523132048|\n",
      "|          65|1178.1656643464019|\n",
      "|         190| 1178.053061669209|\n",
      "|          25|1177.7415366298758|\n",
      "+------------+------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Lazy transformation\n",
    "aggregated_df = df.groupBy(\"PULocationID\") \\\n",
    "    .agg(avg(\"trip_time\").alias(\"avg_trip_duration\")) \\\n",
    "    .orderBy(\"avg_trip_duration\", ascending=False)\n",
    "\n",
    "# Action that trigger the transformations\n",
    "aggregated_df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462ad4b-63b1-463a-995b-1edaf7656a41",
   "metadata": {},
   "source": [
    "---\n",
    "### **Parallelism in Spark**\n",
    "In Spark, we can parallelize the execution and take advantage of parallel processing where each cluster node will take care of a portion of the computation. \n",
    "We can achieve this using the `repartition` command on top of the Dataframe. In the commands below we can check the number of partitions we have in the Dataframe before and after performing a `repartition` (as this is a transformation, it is lazy so, before any action, it will not be executed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6b45cc-6aa6-4afa-98db-eebd3f701c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default partitions: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions after repartitioning: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Default partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "df_repartitioned = df.repartition(8)\n",
    "print(\"Partitions after repartitioning:\", df_repartitioned.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167ea39-9dab-471b-a874-7105a5e9dbbd",
   "metadata": {},
   "source": [
    "---\n",
    "## **Using SparkSQL**\n",
    "SparkSQL is a Spark component that we can use in order to interact with the data using SQL queries. For this, we can register our Dataframe as a virtual table (using the command `createOrReplaceTempView`) and then run a SQL query in order to interact with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058f79d1-2082-4d3e-9447-1eb61f54d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repartitioned.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77947433-ddb0-4a81-ac32-cef9d8c9b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|PULocationID|     avg_trip_time|\n",
      "+------------+------------------+\n",
      "|          29| 945.4347455133862|\n",
      "|          26| 968.7638147239832|\n",
      "|          65|1178.2978641720947|\n",
      "|         191| 962.0914268218004|\n",
      "|         222| 994.9696597525473|\n",
      "|         243|1039.3799716796468|\n",
      "|          19| 969.8039161988224|\n",
      "|          54|1119.7924583171773|\n",
      "|         113|1100.4041809808002|\n",
      "|         167| 993.6639540449976|\n",
      "|         112|1050.7473704364797|\n",
      "|         155|1070.4930071339325|\n",
      "|         241| 955.1747939662611|\n",
      "|         237|1097.6257803802412|\n",
      "|          22| 990.1286824214957|\n",
      "|         198|1073.9567761319909|\n",
      "|         196|1001.4254911461545|\n",
      "|         130| 964.8507134546948|\n",
      "|           7| 987.2249549891809|\n",
      "|          77|1008.1851713859911|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = spark.sql(\"\"\"\n",
    "    SELECT PULocationID, AVG(trip_time) AS avg_trip_time\n",
    "    FROM trips\n",
    "    WHERE trip_time > 60\n",
    "    GROUP BY PULocationID\n",
    "\"\"\")\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba0c3b-8f53-4bb1-acd4-607c7139970d",
   "metadata": {},
   "source": [
    "---\n",
    "### **Write Data back to disk**\n",
    "Spark also allow us to write the data back to disk. When we perform this, each Executor will write a portion of the data in the disk. We will perform a write operation without applying any repartition (using the default one) and we will check how many files are written to disc. After that we will perform a repartition on the data and apply the write again and check how many files do we have (we should have one per partition defined by us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc302d2a-75d7-4713-8e90-7de2374e2cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Read again the data into the Dataframe in order to avoid any previously defined transformations\n",
    "df = spark.read.parquet('file:///opt/spark/data/*.parquet')\n",
    "\n",
    "# This command will write the data using the default partitions\n",
    "#df.write.mode(\"overwrite\").parquet(\"file:///opt/spark/data/test/default_partitions\")\n",
    "\n",
    "# This command will repartition the data and write it to disk\n",
    "# After this, we should see under the folder test/custom_partitions 8 parquet files (ignore the files with extension .parquet.crc)\n",
    "df_repartitioned = df.repartition(8)\n",
    "df_repartitioned.write.mode(\"overwrite\").parquet(\"file:///opt/spark/data/test/custom_partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784f4cf-fc17-47ee-b996-b6925560d548",
   "metadata": {},
   "source": [
    "After executing the previous commands, you should be able to see the data like in the image below\n",
    "![text](../static/data_repartition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af63160-3668-497b-807e-82bbd4c24ed1",
   "metadata": {},
   "source": [
    "---\n",
    "### **Stop Spark Application**\n",
    "After reaching the end of your application, you should stop it in order to free the resources that were allocated to it. Please run the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d51a35-3412-484f-b847-737caf81ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
