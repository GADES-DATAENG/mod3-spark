{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4657f9d1-c83b-47c8-a455-36361c60b5c0",
   "metadata": {},
   "source": [
    "# Introduction to PySpark Basics\n",
    "\n",
    "## About This Notebook\n",
    "\n",
    "This notebook was designed to provide a hands-on introduction to **PySpark**, the Python API for **Apache Spark**, a powerful distributed data processing framework. The primary goal is to demonstrate how PySpark works, including key concepts like lazy transformations, parallel processing, and SparkSQL for querying data.\n",
    "\n",
    "---\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. **Spark Basics**: How to create a Spark session and load data into Spark DataFrames.\n",
    "2. **Lazy Transformations**: Understand how Spark optimizes execution by delaying computations until an action is triggered.\n",
    "3. **Actions vs. Transformations**: Learn the difference between these two core operations.\n",
    "4. **Parallelism in Spark**: See how Spark distributes data and processing across multiple nodes for better performance.\n",
    "5. **SparkSQL**: Use SQL queries to analyze data in Spark DataFrames and leverage the full power of distributed processing.\n",
    "\n",
    "---\n",
    "\n",
    "### Cluster Setup\n",
    "\n",
    "For demonstration purposes, this notebook connects to a local Spark cluster running in cluster mode. Spark is configured to utilize multiple worker nodes to showcase distributed data processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690474a6-ab5f-40f5-b58a-80351f8f05d6",
   "metadata": {},
   "source": [
    "---\n",
    "### **Start by creating a PySpark application that will be used accross the entire Notebook**\n",
    "After running this command, the Spark Application will be up and running and you can check it by going to the [http://localhost:4040](http://localhost:4040) page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6343c28-de3f-48d6-862e-ccab37456717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with more detailed configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkTest\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"29417\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dfbe2-aa70-4056-b23f-e68fa9b78e7a",
   "metadata": {},
   "source": [
    "---\n",
    "### **Now let's load the dataset files into a PySpark Dataframe and check its schema as well as the first 10 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28920d2f-e422-49ee-9758-905b3b82c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset into a PySpark Dataframe\n",
    "# As the file path is *parquet, PySpark will read all the files with the .parquet extension\n",
    "df = spark.read.parquet('file:///opt/spark/data/*.parquet')\n",
    "\n",
    "# Print the dataframe schema inferred from the .parquet files\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first 10 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e79c56-77bb-452e-b25d-07d22c341f87",
   "metadata": {},
   "source": [
    "---\n",
    "### **Transformations and Actions**\n",
    "In Spark we have these two fundamental types of operations:\n",
    "1. **Transformations**: Operations that run on RDDs, Dataframes or Datasets that produce a new distributed dataset from the existing one. They are lazy, which means that product an execution plan and does not actually execute the operation. The transformation (or transformations) applied on a Dataframe are just executed once an action is performed on the Dataframe. Examples are `map`, `filter`, `groupBy`, `join`, `repartition`\n",
    "2. **Actions**: Operations that trigger the execution of transformations and produce a value as return and not a new dataset or write the result to the storage. Actions are the operations that trigger data computation. Examples are `collect`, `count`, `show`, `first`, `write`\n",
    "\n",
    "In the next block, you will see how transformations and actions interact between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce58d8-866c-4ff6-89fb-f94029d7dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a transformation example. It will select just 2 columns from the dataframe (pickup_location and trip_duration)\n",
    "# It will also filter the dataset by the rows where trip_duration > 30\n",
    "# After executing this command, no computation will be triggered and you can verify this in the Spark Application UI\n",
    "# You will not see any job beeing triggered in the Spark Application UI after running this command\n",
    "\n",
    "transformed_df = df.select(\"PULocationID\", \"trip_time\").filter(df.trip_time > 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261862b1-5353-4063-a03d-676be14dede9",
   "metadata": {},
   "source": [
    "Only after running an action on top of the `transformed_df` the transformation will be applied on the data. Just run the next command and you will see the computation being applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab75ca-f784-4cea-8348-405bfb82d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c97cf7-d642-4810-8c0e-d4613f523f82",
   "metadata": {},
   "source": [
    "---\n",
    "### **Aggregations**\n",
    "The next command will do some aggregation on top of the Dataframe:\n",
    "1. **Group**: Aggregate the data on top of `pickup_location`\n",
    "2. **Average**: Calculate the average on top of `trip_duration` and give an alias of `avg_duration`\n",
    "3. **Order**: The last transformation is ordering the resultset by `avg_duration` descending (`ascending=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390a0f9-6f99-4e31-bcd5-0f21058fd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Lazy transformation\n",
    "aggregated_df = df.groupBy(\"PULocationID\") \\\n",
    "    .agg(avg(\"trip_time\").alias(\"avg_trip_duration\")) \\\n",
    "    .orderBy(\"avg_trip_duration\", ascending=False)\n",
    "\n",
    "# Action that trigger the transformations\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462ad4b-63b1-463a-995b-1edaf7656a41",
   "metadata": {},
   "source": [
    "---\n",
    "### **Parallelism in Spark**\n",
    "In Spark, we can parallelize the execution and take advantage of parallel processing where each cluster node will take care of a portion of the computation. \n",
    "We can achieve this using the `repartition` command on top of the Dataframe. In the commands below we can check the number of partitions we have in the Dataframe before and after performing a `repartition` (as this is a transformation, it is lazy so, before any action, it will not be executed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b45cc-6aa6-4afa-98db-eebd3f701c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Default partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "df_repartitioned = df.repartition(8)\n",
    "print(\"Partitions after repartitioning:\", df_repartitioned.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167ea39-9dab-471b-a874-7105a5e9dbbd",
   "metadata": {},
   "source": [
    "---\n",
    "## **Using SparkSQL**\n",
    "SparkSQL is a Spark component that we can use in order to interact with the data using SQL queries. For this, we can register our Dataframe as a virtual table (using the command `createOrReplaceTempView`) and then run a SQL query in order to interact with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f79d1-2082-4d3e-9447-1eb61f54d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77947433-ddb0-4a81-ac32-cef9d8c9b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT PULocationID, trip_time\n",
    "    FROM trips\n",
    "    WHERE trip_time > 60\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba0c3b-8f53-4bb1-acd4-607c7139970d",
   "metadata": {},
   "source": [
    "---\n",
    "### **Write Data back to disk**\n",
    "Spark also allow us to write the data back to disk. When we perform this, each Executor will write a portion of the data in the disk. We will perform a write operation without applying any repartition (using the default one) and we will check how many files are written to disc. After that we will perform a repartition on the data and apply the write again and check how many files do we have (we should have one per partition defined by us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc302d2a-75d7-4713-8e90-7de2374e2cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again the data into the Dataframe in order to avoid any previously defined transformations\n",
    "df = spark.read.parquet('file:///opt/spark/data/*.parquet')\n",
    "\n",
    "# This command will write the data using the default partitions\n",
    "df.write.mode(\"overwrite\").parquet(\"file:///opt/spark/data/test/default_partitions\")\n",
    "\n",
    "# This command will repartition the data and write it to disk\n",
    "# After this, we should see under the folder test/custom_partitions 8 parquet files (ignore the files with extension .parquet.crc)\n",
    "df_repartitioned = df.repartition(8)\n",
    "df_repartitioned.write.mode(\"overwrite\").parquet(\"file:///opt/spark/data/test/custom_partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784f4cf-fc17-47ee-b996-b6925560d548",
   "metadata": {},
   "source": [
    "After executing the previous commands, you should be able to see the data like in the image below\n",
    "![text](../static/data_repartition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af63160-3668-497b-807e-82bbd4c24ed1",
   "metadata": {},
   "source": [
    "---\n",
    "### **Stop Spark Application**\n",
    "After reaching the end of your application, you should stop it in order to free the resources that were allocated to it. Please run the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d51a35-3412-484f-b847-737caf81ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
